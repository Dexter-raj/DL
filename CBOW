import numpy as np
import re
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, Dense, Lambda
import tensorflow as tf

text = """Deep learning (also known as deep structured learning) is part of a broader family of machine learning methods based on artificial neural networks with representation learning. 
Learning can be supervised, semi-supervised or unsupervised. 
Deep-learning architectures such as deep neural networks, deep belief networks, deep reinforcement learning, recurrent neural networks, convolutional neural networks and Transformers have been applied to fields including computer vision, speech recognition, natural language processing, machine translation, bioinformatics, drug design, medical image analysis, climate science, material inspection and board game programs, where they have produced results comparable to and in some cases surpassing human expert performance."""

# Split & clean sentences
sentences = [re.sub('[^A-Za-z0-9]+', ' ', s).lower().strip() for s in text.split('.') if s.strip()]

# -----------------------------
# 2️⃣ Tokenization
# -----------------------------
tokenizer = Tokenizer()
tokenizer.fit_on_texts(sentences)
seqs = tokenizer.texts_to_sequences(sentences)

index_to_word = {v: k for k, v in tokenizer.word_index.items()}
word_to_index = tokenizer.word_index
vocab_size = len(word_to_index) + 1

# -----------------------------
# 3️⃣ Build Context-Target Pairs
# -----------------------------
contexts, targets = [], []
context_size = 2

for seq in seqs:
    for i in range(context_size, len(seq) - context_size):
        contexts.append([seq[i - 2], seq[i - 1], seq[i + 1], seq[i + 2]])
        targets.append(seq[i])

X, Y = np.array(contexts), np.array(targets)

# -----------------------------
# 4️⃣ Define and Train Model
# -----------------------------
model = Sequential([
    Embedding(vocab_size, 10, input_length=4),
    Lambda(lambda x: tf.reduce_mean(x, axis=1)),
    Dense(256, activation='relu'),
    Dense(512, activation='relu'),
    Dense(vocab_size, activation='softmax')
])

model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
history=model.fit(X, Y, epochs=80, verbose=1)

import seaborn as sns
sns.lineplot(model.history.history)

from sklearn.decomposition import PCA

embeddings = model.get_weights()[0]

# -----------------------------
# 5️⃣ Predict Next Word for Test Sentences
# -----------------------------
test_sentences = [
    "known as structured learning",
    "transformers have applied to",
    "where they produced results",
    "cases surpassing expert performance"
]

for s in test_sentences:
    words = s.lower().split()
    seq = [word_to_index.get(w, 0) for w in words]
    pred = np.argmax(model.predict(np.array([seq])), axis=-1)[0]
    print(f"Input: {s}")
    print(f"Predicted next word: {index_to_word.get(pred, '<UNK>')}\n")


